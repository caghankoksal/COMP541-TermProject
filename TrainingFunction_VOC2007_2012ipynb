{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The paper recommends training for 80000 iterations at the initial learning rate.\\nThen, it is decayed by 90% (i.e. to a tenth) for an additional 20000 iterations, twice. With the paper's batch size of 32, this means that the learning rate is decayed by 90% once after the 154th epoch and once more after the 193th epoch, and training is stopped after 232 epochs. I followed this schedule.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "\n",
    "include(\"vgg.jl\")\n",
    "include(\"layers.jl\")\n",
    "include(\"helpers.jl\")\n",
    "include(\"dataset.jl\")\n",
    "include(\"ssd4.jl\")\n",
    "include(\"utilities.jl\")\n",
    "include(\"train.jl\")\n",
    "include(\"transformations.jl\")\n",
    "include(\"config.jl\")\n",
    "using Dates\n",
    "\n",
    "\n",
    "\n",
    "function initopt!(model::SSD300, startLr = 1e-3,gclip = 0)\n",
    "    for par in params(model)\n",
    "        # Authors initialzied learning rates of bias 2x of weights learning rate\n",
    "        if size(par)[1:2] == (1,1) && size(par)[end] == 1 #bias\n",
    "            par.opt = Momentum(;lr=2*startLr, gclip=0, gamma = 0.9)\n",
    "        else\n",
    "            par.opt = Momentum(;lr=startLr, gclip=0,gamma = 0.9)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "   \n",
    "function currentTime()\n",
    "    dt = now()\n",
    "    return Dates.format(dt, \"yyyy-mm-dd_HH:MM:SS\")\n",
    "end\n",
    "function lrdecay!(model::SSD300, decay = 0.1)\n",
    "    for param in params(model)\n",
    "        param.opt.lr = param.opt.lr*decay\n",
    "    end\n",
    "end\n",
    "\n",
    "function mytrain!(model::SSD300, train_data, test_data::PascalVOC,epochs\n",
    "                  ;period::Int=2000, learning_rate_decay = 0.1 )\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    iteration = 0\n",
    "    curTime = currentTime()\n",
    "    tr_loss = 0\n",
    "    tst_loss = 0\n",
    "    println(\"Training has started \", \"length train_data : \",length(train_data), \" Expected epochs : \",epochs)\n",
    "    \n",
    "    filename=\"log_ssd300_2012_VOC2007_2012trainval__l1_batch_$batch_size-iteration_$iterations-$curTime.txt\"\n",
    "    for epoch in 1:epochs\n",
    "        start_epoch_time = now()\n",
    "        println(\"Epoch : \", epoch, \" has started\")\n",
    "       \n",
    "        for (x,bounding_boxes,labels,_) in train_data\n",
    "            \n",
    "            if iteration == 80000|| (iteration > 80000 && iteration % 20000 == 0)\n",
    "\n",
    "                lrdecay!(model,learning_rate_decay)\n",
    "\n",
    "                println(\"Learning_rate decay is applied\")\n",
    "                curLr = params(model)[1].opt.lr\n",
    "                open(\"/kuacc/users/ckoksal20/log_files/$filename\", \"a\") do f\n",
    "                    write(f, \"Learning rate changed to $curLr epoch $epoch \")\n",
    "                end\n",
    "                curTime = currentTime()\n",
    "                Knet.save(\"/kuacc/users/ckoksal20/trained_models/model_ssd300_l1_VOC2007_VOC2012trainval_cur_iteration-$iteration-$curTime.jld2\",\"model_$iteration\",model)\n",
    "            end\n",
    "\n",
    "\n",
    "            if iteration%period == 0\n",
    "                tr_loss = 0\n",
    "                tr_loss = model(train_data) \n",
    "                tst_loss = model(test_data)\n",
    "                println(\"Iteration \", iteration, \" training loss : \",tr_loss, \" test_loss : \",tst_loss)\n",
    "                open(\"/kuacc/users/ckoksal20/log_files/$filename\", \"a\") do f\n",
    "                    write(f, \"Iteration: $iteration training loss $tr_loss test_loss $tst_loss \\n\")\n",
    "                end\n",
    "                Knet.save(\"/kuacc/users/ckoksal20/trained_models/model_ssd300_l1_VOC2007_VOC2012trainval_cur_iteration-$iteration-$curTime.jld2\",\"model_$iteration\",model)\n",
    "                append!(train_loss,tr_loss )\n",
    "                append!(test_loss,tst_loss )\n",
    "            end\n",
    "            \n",
    "            momentum!(ssd300,[(x,bounding_boxes,labels)])\n",
    "            iteration +=1\n",
    "        end\n",
    "    end_epoch_time = now()\n",
    "    elapsed_time = ((end_epoch_time -start_epoch_time).value)/1000/60\n",
    "        \n",
    "    println(\"Elapsed time $elapsed_time min epoch : $epoch\\n\")    \n",
    "    open(\"/kuacc/users/ckoksal20/log_files/$filename\", \"a\") do f\n",
    "        write(f, \"Elapsed time $elapsed_time min epoch : $epoch\\n\")\n",
    "        #write(f,\"Iteration: $iteration, training loss : $tr_loss, test_loss : $tst_loss \\n\")\n",
    "    end\n",
    "    end\n",
    "        \n",
    "    return 0:period:iteration, train_loss, test_loss\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The paper recommends training for 80000 iterations at the initial learning rate.\n",
    "Then, it is decayed by 90% (i.e. to a tenth) for an additional 20000 iterations, twice. With the paper's batch size of 32, this means that the learning rate is decayed by 90% once after the 154th epoch and once more after the 193th epoch, and training is stopped after 232 epochs. I followed this schedule.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only New Pretrained VGG16 constructor is called\n",
      "Training has started length train_data : 518 Expected epochs : 232.0\n",
      "Epoch : 1.0 has started\n",
      "Dataset Loss is being calculated\n",
      "Dataset Loss is being calculated\n",
      "Iteration 0 training loss : 133.7157950366736 test_loss : 133.78134164464478\n",
      "Elapsed time 23.45685 min epoch : 1.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = PascalVOC(\n",
    "    [trainval_VOC2007,trainval_VOC2012],\n",
    "    [annotation_path_trainval_VOC2007,annotation_path_train_VOC2012],\n",
    "    [images_path_trainval_VOC2007,images_path_train_VOC2012],\"TEST\",dtype = atype, batchsize =batch_size,\n",
    "shuffle =true, multi_dataset = true)\n",
    "\n",
    "test_VOC2007_dateset = PascalVOC(test_VOC2007, annotation_path_test_VOC2007, images_path_test_VOC2007,\n",
    "    \"TEST\", dtype=atype,batchsize=batch_size, shuffle=true, multi_dataset = false)\n",
    "\n",
    "ssd300 = SSD300(pretrained = false)\n",
    "initopt!(ssd300)\n",
    "\n",
    "\n",
    "\n",
    "iterations = 120000\n",
    "epochs = round(iterations/length(train_dataset))\n",
    "\n",
    "iters, trnloss, tstloss = mytrain!(ssd300,train_dataset,test_VOC2007_dateset,epochs)\n",
    "using Plots; default(fmt = :png, ls=:auto)\n",
    "plot(iters, trnloss, label=\"train\",xlabel =\"iterations\", ylabel = \"loss\")\n",
    "plot!(iters, tstloss, label = \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
